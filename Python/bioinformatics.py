def blast_import(file_path, delim):
    '''
    imports individual blast text file and adds column names
    file_path - path to text file
    delim - type of delimiter e.g. "\t", ","
    '''
    dataframe = pd.read_table(file_path, sep = delim, header = None)
    blast_colnames = ["query seqid", "subject seqid", "identity match %", "length", "mismatches", "gap open",
                  "qstart", "qend", "alignment start", "alignment end",
                  "evalue", "bitscore"]
    dataframe.columns = blast_colnames
    return(dataframe)

def blast_concat(file_end, out_name):
    '''
    imports multiple blast files, such as individual files from blasting MAGs
    returns all the data in one pandas dataframe with a column for the file name
    assumes files are in the same folder as the script

    inputs:
        folder - folder where the blast files are
        file_end - common file ending to search for such as "*_blast_cphA.txt"
            ideally file names should a similar structure to this: seqname_searchtype_genequery.txt
        out_name - output name, will save to current working directory unless another is specified
    example:
        blast_concat("/","*_blast_cphA.txt","all_blast.csv")

    '''
    import glob
    import csv

    alldata_list = []

    filenames = glob.glob(file_end)

    for name in filenames:
        with open(name) as read_file:
            reader = csv.reader(read_file, delimiter="\t")
            file_list = list(reader)
            temp_end = file_end.replace("*","") # string with file ending to remove
            seqid = name.replace(temp_end,"")
        for l in file_list:
            l.append(seqid)
            alldata_list.append(l)
    with open(out_name, "w", newline="") as write_file:
        writer = csv.writer(write_file)
        writer.writerows(alldata_list)

def blast_columns(dataframe):
    '''
    adds blast column names to blast dataframe from text file
    must import dataframe first from text - more limited than blast_import
    '''
    if isinstance(dataframe,pd.DataFrame) == True:
        blast_colnames = ["query seqid", "subject seqid", "identity match %", "length", "mismatches", "gap open",
                      "qstart", "qend", "alignment start", "alignment end",
                      "evalue", "bitscore"]
        dataframe.columns = blast_colnames
        return(dataframe.head())
    else:
        dataframe = None
        print("Provide a pandas dataframe")

def fasta_concat(directory, file_end, out_dir_and_name):
    '''
    creates one fasta file from many individual fasta files
    useful for creating a custom blast db

    inputs:
        directory - location where individual fasta files are saved
        file_end - common file name ending
        out_dir_and_name - output directory and file name

    example:
        fasta_concat("./Sequences", "/*cphA.fasta", "./Database files/cphA.fasta")
    '''
    import glob
    from Bio import SeqIO

    filenames = glob.glob(directory + file_end)
    with open(out_dir_and_name, "w") as w_file:
        for file in filenames:
            record = SeqIO.parse(file, "fasta")
            SeqIO.write(record, w_file, "fasta")

def checkm_qa_parse(file_path,out_path):
    '''
    creates a json file from a checkM qa file (https://github.com/Ecogenomics/CheckM/wiki/Reported-Statistics#qa)
    the qa file is a tsv, which can be hard to work with directly
    makes a nested dictionary, where the format is {bin id: {qa statistic: value, ...}}

    inputs:
        file_path = where the tsv file is
        out_path = location and name of output file, make sure this is a .json file

    '''
    import json

    path = file_path
    qa = open(path).read()
    # cleaning - parsing the file into a list of lines
    qa_lines = qa.split("\n")
    # removing last list entry if its blank
    if qa_lines[-1] == "":
        qa_lines.pop()
    else:
        pass
    # save to dict
    qa_dict = {}
    for line in qa_lines:
        line_temp = line.split("\t")
        data = json.loads(line_temp[1].replace("'",'"'))
        qa_dict[line_temp[0]] = data
    # export to json
    with open("checkm_qa.json", "w", encoding='utf-8') as f:
        json.dump(qa_dict, f, ensure_ascii=False)

def tsv_move(tsv, out_dir):
    """
    copy tsv from individual prokka folders to one folder for parsing

    inputs:
        tsv = path to find tsv folders, if file is saved in prokka dir, use "./*/*.tsv"
        out_dir = where tsv files should go
    """
    import glob
    from shutil import copy

    files = glob.glob(tsv)

    for i in files:
        copy(i,out_dir)

def prokka_search(directory, search_term):
    '''
    search tsvs generated by prokka for a particular search term such as a gene of interest
    inputs:
        directory = where the tsv files are located
        search_term = what you're looking for; example - "cphA"
    outputs:
        result_df = pandas dataframe of prokka info and bin
    '''
    import glob
    import pandas as pd

    match_list = []
    line_list = []
    replace_string = directory.replace("/*.tsv","") + "\\"
    filenames = glob.glob(directory)

    for name in filenames:
        with open(name) as read_file: # open each file
            all_lines = read_file.readlines() # read the lines in
            for line in all_lines:
                if search_term in line: # find search term in line
                    match_list.append(name)
                    line_list.append(line)
                else:
                    pass

    for i in range(0,len(match_list)):
        match_list[i] = match_list[i].replace(replace_string,"")
        match_list[i] = match_list[i].replace(".tsv","")
        line_list[i] = line_list[i].replace("\n","")
    result_df = pd.DataFrame([x.split("\t") for x in line_list], columns=["locus_tag","ftype","length_bp","gene","EC_number","COG","product"])
    result_df["bin"] = match_list

    return result_df
